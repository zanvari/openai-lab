{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a4770d",
   "metadata": {},
   "source": [
    "# üí¨ OpenAI Chat Completions ‚Äì Prompting & Parameters\n",
    "\n",
    "Explore how the OpenAI Chat API works with different roles, temperature settings, and prompt structures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128eb7a1",
   "metadata": {},
   "source": [
    "## üîß Setup\n",
    "Install required libraries and load your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dbd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai tiktoken --quiet\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Optional: load from environment variable\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\", \"sk-...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ddebe1",
   "metadata": {},
   "source": [
    "## üéØ Basic Chat Completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fe2f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4546693",
   "metadata": {},
   "source": [
    "## üé≠ Role Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057033a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "sarcastic = openai.ChatCompletion.create(model=\"gpt-4-turbo\", messages=messages)\n",
    "\n",
    "messages[0][\"content\"] = \"You are a poetic assistant.\"\n",
    "poetic = openai.ChatCompletion.create(model=\"gpt-4-turbo\", messages=messages)\n",
    "\n",
    "print(\"üåÄ Sarcastic:\", sarcastic.choices[0].message['content'])\n",
    "print(\"üå∏ Poetic:\", poetic.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf7368",
   "metadata": {},
   "source": [
    "## üé≤ Sampling Parameters: Temperature & top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd19bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"List three creative uses for a paperclip.\"\n",
    "\n",
    "for temp in [0.2, 0.7, 1.0]:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4-turbo\",\n",
    "        temperature=temp,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    print(f\"üå°Ô∏è Temp {temp}: {response.choices[0].message['content']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410696e",
   "metadata": {},
   "source": [
    "## üß† Chain-of-Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb982ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"If there are 3 red balls and 2 blue balls in a bag, what's the probability of picking a blue one?\"\n",
    "\n",
    "cot_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Let's think step by step.\\n\" + prompt}\n",
    "    ]\n",
    ")\n",
    "print(cot_response.choices[0].message['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c08a3a",
   "metadata": {},
   "source": [
    "## üî¢ Token Counting (with `tiktoken`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3009bd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "tokens = enc.encode(prompt)\n",
    "print(f\"Prompt token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa432016",
   "metadata": {},
   "source": [
    "## ‚úÖ Summary\n",
    "- You tested multiple prompting styles and system roles\n",
    "- You explored temperature and top_p sampling\n",
    "- You tried chain-of-thought prompting\n",
    "- You measured token usage"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
